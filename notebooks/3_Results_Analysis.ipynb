{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Results Analysis & Visualization\n",
    "\n",
    "**Objective:** To evaluate the performance of our trained Synapse agent and compare it against baseline routing strategies.\n",
    "\n",
    "This notebook will:\n",
    "1. Load the trained PPO model.\n",
    "2. Define evaluation logic for different agents (PPO, OSPF, Random).\n",
    "3. Run a series of evaluation episodes for each agent to collect performance metrics.\n",
    "4. Generate plots comparing the agents on key metrics like latency, path length, and success rate. These plots are intended for the research paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "from synapse.network_env import NetworkRoutingEnv\n",
    "from synapse.baselines import ospf_routing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Model and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPOLOGY_FILE = '../data/topologies/nsfnet.gml'\n",
    "MODEL_SAVE_PATH = '../models/synapse_ppo_nsfnet.zip'\n",
    "NUM_EVAL_EPISODES = 200 # Use a significant number of episodes for stable results\n",
    "\n",
    "# Create the evaluation environment\n",
    "eval_env = NetworkRoutingEnv(graph_file=TOPOLOGY_FILE)\n",
    "\n",
    "# Load the trained agent\n",
    "try:\n",
    "    agent = PPO.load(MODEL_SAVE_PATH)\n",
    "    print(\"Trained PPO agent loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Model not found at {MODEL_SAVE_PATH}\")\n",
    "    print(\"Please run Notebook 2 to train and save the model first.\")\n",
    "    agent = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Evaluation Logic\n",
    "\n",
    "We will create a standardized evaluation loop that can run any given policy (our RL agent, OSPF, or random) and collect metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(agent_type, model, env, num_episodes):\n",
    "    results = []\n",
    "    \n",
    "    for _ in tqdm(range(num_episodes), desc=f\"Evaluating {agent_type}\"):\n",
    "        obs, info = env.reset()\n",
    "        terminated = False\n",
    "        episode_latency = 0\n",
    "        path_len = 0\n",
    "        \n",
    "        # For OSPF, calculate the full path once at the beginning\n",
    "        if agent_type == 'OSPF':\n",
    "            source = obs['current_node']\n",
    "            dest = obs['destination_node']\n",
    "            ospf_path = ospf_routing(env.graph, source, dest, weight='weight')\n",
    "        \n",
    "        while not terminated:\n",
    "            if agent_type == 'PPO':\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "            elif agent_type == 'Random':\n",
    "                action = env.action_space.sample()\n",
    "            elif agent_type == 'OSPF':\n",
    "                current_node = obs['current_node']\n",
    "                if not ospf_path or current_node not in ospf_path:\n",
    "                    action = env.action_space.sample() # Fallback if path is broken\n",
    "                else:\n",
    "                    current_idx = ospf_path.index(current_node)\n",
    "                    next_hop = ospf_path[current_idx + 1]\n",
    "                    neighbors = list(env.graph.neighbors(current_node))\n",
    "                    action = neighbors.index(next_hop)\n",
    "            \n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            if terminated:\n",
    "                success = (obs['current_node'] == obs['destination_node'])\n",
    "                episode_latency = info['total_latency']\n",
    "                path_len = len(info['path']) -1\n",
    "                results.append({'latency': episode_latency, 'path_length': path_len, 'success': success})\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run Evaluations\n",
    "\n",
    "Now we run the evaluation for all three strategies. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Evaluate PPO Agent\n",
    "if agent:\n",
    "    ppo_results = evaluate_policy('PPO', agent, eval_env, NUM_EVAL_EPISODES)\n",
    "    ppo_results['Agent'] = 'PPO (Synapse)'\n",
    "    all_results = pd.concat([all_results, ppo_results])\n",
    "\n",
    "# Evaluate Random Agent\n",
    "random_results = evaluate_policy('Random', None, eval_env, NUM_EVAL_EPISODES)\n",
    "random_results['Agent'] = 'Random'\n",
    "all_results = pd.concat([all_results, random_results])\n",
    "\n",
    "# Evaluate OSPF Agent\n",
    "ospf_results = evaluate_policy('OSPF', None, eval_env, NUM_EVAL_EPISODES)\n",
    "ospf_results['Agent'] = 'OSPF (Shortest Path)'\n",
    "all_results = pd.concat([all_results, ospf_results])\n",
    "\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "all_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Analyze and Plot Results\n",
    "\n",
    "This is where we generate the final figures for the paper. We'll compare the agents on three key metrics:\n",
    "1.  **Average Latency:** Lower is better. This is our primary optimization goal.\n",
    "2.  **Average Path Length:** The number of hops. We want to see if our agent finds longer but faster paths.\n",
    "3.  **Success Rate:** The percentage of packets that successfully reach their destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "summary = all_results.groupby('Agent').agg(\n",
    "    avg_latency=('latency', 'mean'),\n",
    "    avg_path_length=('path_length', 'mean'),\n",
    "    success_rate=('success', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "summary['success_rate'] = summary['success_rate'] * 100 # Convert to percentage\n",
    "\n",
    "print(\"--- Performance Summary ---\")\n",
    "print(summary)\n",
    "\n",
    "# --- Generate Plots ---\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle('Routing Performance Comparison', fontsize=16)\n",
    "\n",
    "# Plot 1: Average Latency\n",
    "sns.barplot(x='Agent', y='avg_latency', data=summary, ax=axes[0], palette='viridis')\n",
    "axes[0].set_title('Average Packet Latency')\n",
    "axes[0].set_ylabel('Latency (simulation units)')\n",
    "\n",
    "# Plot 2: Average Path Length\n",
    "sns.barplot(x='Agent', y='avg_path_length', data=summary, ax=axes[1], palette='plasma')\n",
    "axes[1].set_title('Average Path Length (Hops)')\n",
    "axes[1].set_ylabel('Number of Hops')\n",
    "\n",
    "# Plot 3: Success Rate\n",
    "sns.barplot(x='Agent', y='success_rate', data=summary, ax=axes[2], palette='magma')\n",
    "axes[2].set_title('Packet Delivery Success Rate')\n",
    "axes[2].set_ylabel('Success Rate (%)')\n",
    "axes[2].set_ylim(0, 101)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Discussion of Results\n",
    "\n",
    "**Expected Outcome:**\n",
    "*   **Random:** Will perform the worst across all metrics, with very high latency, long paths (due to loops), and a low success rate.\n",
    "*   **OSPF:** Will have the shortest path length by definition. However, because it is unaware of traffic, it will frequently send packets into congested areas, leading to high latency.\n",
    "*   **PPO (Synapse):** The trained agent should achieve the **lowest average latency**. It will likely accomplish this by sometimes choosing slightly longer paths (higher hop count than OSPF) to deliberately avoid congested links. Its success rate should be near 100%.\n",
    "\n",
    "This result—sacrificing the shortest path for a faster overall journey—is the core demonstration of an intelligent, traffic-aware routing system and will be a key point in your paper's analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}